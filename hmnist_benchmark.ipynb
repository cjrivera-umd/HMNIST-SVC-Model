{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-16 13:01:55.383717: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Images\n",
    "### Converting JPEG images into rows of pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving ground truth data (skin lesion diagnoses)\n",
    "ground_truth_train_path = './archive/ISIC2018_Task3_Training_GroundTruth.csv'\n",
    "ground_truth_train_df = pd.read_csv(ground_truth_train_path)\n",
    "\n",
    "ground_truth_test_path = './archive/ISIC2018_Task3_Test_GroundTruth.csv'\n",
    "ground_truth_test_df = pd.read_csv(ground_truth_test_path)\n",
    "\n",
    "ground_truth_df = pd.concat([ground_truth_train_df, ground_truth_test_df], axis=0)\n",
    "CLASSES = ground_truth_df.columns.to_list()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working through: ./archive/HAM10000_images_part_1/\n",
      "Working through: ./archive/HAM10000_images_part_2/\n",
      "Working through: ./archive/ISIC2018_Task3_Test_Input/\n",
      "Shape of training data: (10016, 2359)\n",
      "Shape of testing data: (1513, 2359)\n"
     ]
    }
   ],
   "source": [
    "# Converting images to pixel numpy arrays\n",
    "PIXELS = 28\n",
    "IMG_DIRS_TRAIN = ['./archive/HAM10000_images_part_1/', './archive/HAM10000_images_part_2/']\n",
    "IMG_DIRS_TEST = ['./archive/ISIC2018_Task3_Test_Input/']\n",
    "\n",
    "header = np.array([f'pixel{i:04}' for i in range(PIXELS * PIXELS * 3)] + CLASSES)\n",
    "data_train, data_test = [header], [header]\n",
    "for data, IMG_DIRS in zip([data_train, data_test], [IMG_DIRS_TRAIN, IMG_DIRS_TEST]):\n",
    "    for IMG_DIR in IMG_DIRS:\n",
    "        print(f'Working through: {IMG_DIR}')\n",
    "        for img in os.listdir(IMG_DIR):\n",
    "            # Getting pixel values from original jpg image\n",
    "            bgr_array = cv2.imread(os.path.join(IMG_DIR, img), cv2.IMREAD_COLOR)\n",
    "            bgr_pil = Image.fromarray(bgr_array)\n",
    "            bgr_resized = np.array(bgr_pil.resize((PIXELS, PIXELS)))\n",
    "            bgr_array = bgr_resized.flatten()\n",
    "\n",
    "            # Restructuring from BGR to RGB\n",
    "            rgb_list = []\n",
    "            for i in range(PIXELS ** 2):\n",
    "                b, g, r = bgr_array[(3 * i):(i * 3) + 3]\n",
    "                rgb_list += [r, g, b]\n",
    "            rgb_array = np.array(rgb_list)\n",
    "\n",
    "            # Getting classification\n",
    "            img_id = img.split('.')[0]\n",
    "            classification_array = ground_truth_df.loc[ground_truth_df['image'] == img_id, CLASSES].to_numpy()\n",
    "            instance_array = np.append(rgb_array, classification_array)\n",
    "\n",
    "            # Adding image data to data list\n",
    "            data.append(instance_array)\n",
    "\n",
    "data_array_train = np.array(data_train)\n",
    "data_array_test = np.array(data_test)\n",
    "print(f'Shape of training data: {data_array_train.shape}')\n",
    "print(f'Shape of testing data: {data_array_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing plotting functionality\n",
    "\n",
    "def plot_image(data):\n",
    "    corrected = data.astype(float).astype(int)\n",
    "    image = corrected.reshape(PIXELS, PIXELS, 3)\n",
    "    plt.imshow(image)\n",
    "\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of pixelated images:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting an example image\n",
    "print('Example of pixelated images:')\n",
    "plot_image(data_array_train[1, :-7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "### Adding variety to data by rotating and flipping images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of HMNIST data: (10015, 2359)\n"
     ]
    }
   ],
   "source": [
    "# Organizing data\n",
    "hmnist_train = data_array_train[1:]\n",
    "print(f'Shape of HMNIST data: {hmnist_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X train (instance matrix): (10015, 28, 28, 3)\n",
      "Shape of y train (labels): (10015,)\n"
     ]
    }
   ],
   "source": [
    "# Extracting images and OHE labels; and also converting to categorical labels\n",
    "X_train = hmnist_train[:, :-7]\n",
    "X_train = X_train.reshape(len(X_train), PIXELS, PIXELS, 3)\n",
    "print(f'Shape of X train (instance matrix): {X_train.shape}')\n",
    "\n",
    "labels_train_ohe = hmnist_train[:, -7:]\n",
    "y_train = np.where(labels_train_ohe == '1.0')[1]\n",
    "print(f'Shape of y train (labels): {y_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rotations finished. Generated 30045 rotated images.\n"
     ]
    }
   ],
   "source": [
    "# Rotating images\n",
    "rotated_instances = { 1: [], 2: [], 3: [] }\n",
    "for rotation in rotated_instances:\n",
    "    for x, y in zip(X_train, y_train):\n",
    "        rotated_image = np.rot90(x, rotation)\n",
    "        flattened_image = rotated_image.flatten()\n",
    "        labeled_instance = np.append(flattened_image, y)\n",
    "        rotated_instances[rotation].append(labeled_instance)\n",
    "\n",
    "total_rotations = 0\n",
    "for rotation in rotated_instances:\n",
    "    total_rotations += len(rotated_instances[rotation])\n",
    "                           \n",
    "print(f'Rotations finished. Generated {total_rotations} rotated images.')\n",
    "\n",
    "# Adding original (non-rotated) images\n",
    "rotated_instances[0] = []\n",
    "for x, y in zip(X_train, y_train):\n",
    "    flattened_image = x.flatten()\n",
    "    labeled_instance = np.append(flattened_image, y)\n",
    "    rotated_instances[0].append(labeled_instance)\n",
    "all_rotated_instances = rotated_instances[0] + rotated_instances[1] + rotated_instances[2] + rotated_instances[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flipping finished. Generated 40060 rotated images.\n",
      "Example of flipped images:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Flipping images\n",
    "flipped_instances = []\n",
    "for rotated_instance in all_rotated_instances:\n",
    "    x, y = rotated_instance[:-1], rotated_instance[-1]\n",
    "    image = x.reshape(PIXELS, PIXELS, 3)\n",
    "    flipped_image = np.fliplr(image)\n",
    "    flattened_image = flipped_image.flatten()\n",
    "    labeled_instance = np.append(flattened_image, y)\n",
    "    flipped_instances.append(labeled_instance)\n",
    "\n",
    "print(f'Flipping finished. Generated {len(flipped_instances)} rotated images.')\n",
    "print('Example of flipped images:')\n",
    "plot_image(flipped_instances[0][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of final instances matrix: (80120, 2353)\n"
     ]
    }
   ],
   "source": [
    "# Combining all data\n",
    "all_instances = all_rotated_instances + flipped_instances\n",
    "all_instances = np.array(all_instances)\n",
    "print(f'Shape of final instances matrix: {all_instances.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Training a CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features matrix: (80120, 28, 28, 3)\n",
      "Training labels vector: (80120,)\n"
     ]
    }
   ],
   "source": [
    "# Reshaping training data\n",
    "X_train_aug, y_train_aug = all_instances[:, :-1], all_instances[:, -1]\n",
    "X_train_aug = X_train_aug.reshape(len(X_train_aug), PIXELS, PIXELS, 3)\n",
    "print(f'Training features matrix: {X_train_aug.shape}')\n",
    "print(f'Training labels vector: {y_train_aug.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling and typing data\n",
    "X_train_aug = X_train_aug.astype(float) / 255.0\n",
    "y_train_aug = y_train_aug.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 64)        9472      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 14, 14, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 128)       73856     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 14, 14, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 7, 7, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 7, 7, 256)         295168    \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 7, 7, 256)         590080    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 3, 3, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2304)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               295040    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 7)                 455       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1419911 (5.42 MB)\n",
      "Trainable params: 1419911 (5.42 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model Implementation\n",
    "DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, padding='same',\n",
    "                        activation='relu', kernel_initializer='he_normal')\n",
    "model = tf.keras.Sequential([\n",
    "    DefaultConv2D(filters=64, kernel_size=7, input_shape=[PIXELS,PIXELS,3]),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    DefaultConv2D(filters=128),\n",
    "    DefaultConv2D(filters=128),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    DefaultConv2D(filters=256),\n",
    "    DefaultConv2D(filters=256),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=128, activation='relu',\n",
    "                          kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu',\n",
    "                          kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(units=7, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='sgd',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2504/2504 [==============================] - 228s 91ms/step - loss: 0.9841 - accuracy: 0.6691\n",
      "Epoch 2/5\n",
      "2504/2504 [==============================] - 224s 89ms/step - loss: 0.8702 - accuracy: 0.6837\n",
      "Epoch 3/5\n",
      "2504/2504 [==============================] - 222s 89ms/step - loss: 0.8168 - accuracy: 0.7009\n",
      "Epoch 4/5\n",
      "2504/2504 [==============================] - 223s 89ms/step - loss: 0.7774 - accuracy: 0.7180\n",
      "Epoch 5/5\n",
      "2504/2504 [==============================] - 220s 88ms/step - loss: 0.7453 - accuracy: 0.7290\n"
     ]
    }
   ],
   "source": [
    "# Training model\n",
    "history = model.fit(X_train_aug, y_train_aug, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X test (instance matrix): (1512, 28, 28, 3)\n",
      "Shape of y test (labels): (1512,)\n"
     ]
    }
   ],
   "source": [
    "# Preparing test data\n",
    "hmnist_test = data_array_test[1:]\n",
    "\n",
    "X_test = hmnist_test[:, :-7].astype(float)\n",
    "X_test = X_test.reshape(len(X_test), PIXELS, PIXELS, 3)\n",
    "print(f'Shape of X test (instance matrix): {X_test.shape}')\n",
    "\n",
    "labels_test_ohe = hmnist_test[:, -7:]\n",
    "y_test = np.where(labels_test_ohe == '1.0')[1]\n",
    "print(f'Shape of y test (labels): {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 1s 22ms/step - loss: 177.2093 - accuracy: 0.5866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[177.2093048095703, 0.5866402387619019]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating model\n",
    "model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
